














import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
import datetime

from library.sb_utils import save_file





ski_data = pd.read_csv('../data/ski_data_step3_features.csv')
ski_data.head().T








big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']


big_mountain.T


ski_data.shape


ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']


ski_data.shape











len(ski_data) * .7, len(ski_data) * .3


X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), 
                                                    ski_data.AdultWeekend, test_size=0.3, 
                                                    random_state=47)


X_train.shape, X_test.shape


y_train.shape, y_test.shape


#Code task 1#
#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'
names_list = ['Name', 'state', 'Region']
names_train = X_train[names_list]
names_test = X_test[names_list]
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape


#Code task 2#
#Check the `dtypes` attribute of `X_train` to verify all features are numeric
X_train.dtypes


#Code task 3#
#Repeat this check for the test split in `X_test`
X_test.dtypes











#Code task 4#
#Calculate the mean of `y_train`
train_mean = y_train.mean()
train_mean





#Code task 5#
#Fit the dummy regressor on the training data
#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments
#Then print the object's `constant_` attribute and verify it's the same as the mean above
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
dumb_reg.constant_














#Code task 6#
#Calculate the R^2 as defined above
def r_squared(y, ypred):
    """R-squared score.
    
    Calculate the R-squared, or coefficient of determination, of the input.
    
    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)
    sum_sq_tot = np.___((y - ybar)**2) #total sum of squares error
    sum_sq_res = np.___((y - ypred)**2) #residual sum of squares error
    R2 = 1.0 - ___ / ___
    return R2





y_tr_pred_ = train_mean * np.ones(len(y_train))
y_tr_pred_[:5]





y_tr_pred = dumb_reg.predict(X_train)
y_tr_pred[:5]





r_squared(y_train, y_tr_pred)








y_te_pred = train_mean * np.ones(len(y_test))
r_squared(y_test, y_te_pred)














#Code task 7#
#Calculate the MAE as defined above
def mae(y, ypred):
    """Mean absolute error.
    
    Calculate the mean absolute error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    abs_error = np.abs(___ - ___)
    mae = np.mean(___)
    return mae


mae(y_train, y_tr_pred)


mae(y_test, y_te_pred)











#Code task 8#
#Calculate the MSE as defined above
def mse(y, ypred):
    """Mean square error.
    
    Calculate the mean square error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    sq_error = (___ - ___)**2
    mse = np.mean(___)
    return mse


mse(y_train, y_tr_pred)


mse(y_test, y_te_pred)





np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])











r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)





mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)





mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)











# train set - sklearn
# correct order, incorrect order
r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)


# test set - sklearn
# correct order, incorrect order
r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)


# train set - using our homebrew function
# correct order, incorrect order
r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)


# test set - using our homebrew function
# correct order, incorrect order
r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)























# These are the values we'll use to fill in any missing values
X_defaults_median = X_train.median()
X_defaults_median





#Code task 9#
#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use
#Assign the results to `X_tr` and `X_te`, respectively
X_tr = X_train.___(___)
X_te = X_test.___(___)








#Code task 10#
#Call the StandardScaler`s fit method on `X_tr` to fit the scaler
#then use it's `transform()` method to apply the scaling to both the train and test split
#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively
scaler = StandardScaler()
scaler.___(X_tr)
X_tr_scaled = scaler.___(X_tr)
X_te_scaled = scaler.___(X_te)





lm = LinearRegression().fit(X_tr_scaled, y_train)





#Code task 11#
#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data
#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively
y_tr_pred = lm.___(X_tr_scaled)
y_te_pred = lm.___(X_te_scaled)





# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
median_r2





#Code task 12#
#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = ___(y_train, y_tr_pred), ___(y_test, y_te_pred)
median_mae





#Code task 13#
#And also do the same using `sklearn`'s `mean_squared_error`
# MSE - train, test
median_mse = ___(___, ___), ___(___, ___)
median_mse











#Code task 14#
#As we did for the median above, calculate mean values for imputing missing values
# These are the values we'll use to fill in any missing values
X_defaults_mean = X_train.___()
X_defaults_mean








X_tr = X_train.fillna(X_defaults_mean)
X_te = X_test.fillna(X_defaults_mean)





scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)





lm = LinearRegression().fit(X_tr_scaled, y_train)





y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)





r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)


mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)


mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)














pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(), 
    LinearRegression()
)


type(pipe)


hasattr(pipe, 'fit'), hasattr(pipe, 'predict')








#Code task 15#
#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.___(___, ___)





y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)





r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)





median_r2


mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)


Compare with your earlier result:


median_mae


mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)





median_mse

















#Code task 16#
#Add `SelectKBest` as a step in the pipeline between `StandardScaler()` and `LinearRegression()`
#Don't forget to tell it to use `f_regression` as its score function
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    ___(___),
    LinearRegression()
)





pipe.fit(X_train, y_train)





y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)


r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)


mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)








#Code task 17#
#Modify the `SelectKBest` step to use a value of 15 for k
pipe15 = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    ___(___, k=___),
    LinearRegression()
)





pipe15.fit(X_train, y_train)





y_tr_pred = pipe15.predict(X_train)
y_te_pred = pipe15.predict(X_test)


r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)


mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)








cv_results = cross_validate(pipe15, X_train, y_train, cv=5)


cv_scores = cv_results['test_score']
cv_scores





np.mean(cv_scores), np.std(cv_scores)





np.round((np.mean(cv_scores) - 2 * np.std(cv_scores), np.mean(cv_scores) + 2 * np.std(cv_scores)), 2)








#Code task 18#
#Call `pipe`'s `get_params()` method to get a dict of available parameters and print their names
#using dict's `keys()` method
pipe.___.keys()





k = [k+1 for k in range(len(X_train.columns))]
grid_params = {'selectkbest__k': k}





lr_grid_cv = GridSearchCV(pipe, param_grid=grid_params, cv=5, n_jobs=-1)


lr_grid_cv.fit(X_train, y_train)


score_mean = lr_grid_cv.cv_results_['mean_test_score']
score_std = lr_grid_cv.cv_results_['std_test_score']
cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]


#Code task 19#
#Print the `best_params_` attribute of `lr_grid_cv`
lr_grid_cv.___


#Code task 20#
#Assign the value of k from the above dict of `best_params_` and assign it to `best_k`
___ = lr_grid_cv.___['selectkbest__k']
plt.subplots(figsize=(10, 5))
plt.errorbar(cv_k, score_mean, yerr=score_std)
plt.axvline(x=best_k, c='r', ls='--', alpha=.5)
plt.xlabel('k')
plt.ylabel('CV score (r-squared)')
plt.title('Pipeline mean CV score (error bars +/- 1sd)');








selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()





#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(___, index=___).___(ascending=___)














#Code task 22#
#Define a pipeline comprising the steps:
#SimpleImputer() with a strategy of 'median'
#StandardScaler(),
#and then RandomForestRegressor() with a random state of 47
RF_pipe = make_pipeline(
    ___(strategy=___),
    ___,
    ___(random_state=___)
)





#Code task 23#
#Call `cross_validate` to estimate the pipeline's performance.
#Pass it the random forest pipe object, `X_train` and `y_train`,
#and get it to use 5-fold cross-validation
rf_default_cv_results = cross_validate(___, ___, ___, cv=___)


rf_cv_scores = rf_default_cv_results['test_score']
rf_cv_scores


np.mean(rf_cv_scores), np.std(rf_cv_scores)








n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]
grid_params = {
        'randomforestregressor__n_estimators': n_est,
        'standardscaler': [StandardScaler(), None],
        'simpleimputer__strategy': ['mean', 'median']
}
grid_params


#Code task 24#
#Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`
#dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)
rf_grid_cv = GridSearchCV(___, param_grid=___, cv=___, n_jobs=-1)


#Code task 25#
#Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments
#to actually start the grid search. This may take a minute or two.
rf_grid_cv.___(___, ___)


#Code task 26#
#Print the best params (`best_params_` attribute) from the grid search
rf_grid_cv.___





rf_best_cv_results = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, cv=5)
rf_best_scores = rf_best_cv_results['test_score']
rf_best_scores


np.mean(rf_best_scores), np.std(rf_best_scores)





#Code task 27#
#Plot a barplot of the random forest's feature importances,
#assigning the `feature_importances_` attribute of 
#`rf_grid_cv.best_estimator_.named_steps.randomforestregressor` to the name `imps` to then
#create a pandas Series object of the feature importances, with the index given by the
#training data column names, sorting the values in descending order
plt.subplots(figsize=(10, 5))
imps = rf_grid_cv.best_estimator_.named_steps.randomforestregressor.___
rf_feat_imps = pd.Series(___, index=X_train.columns).sort_values(ascending=False)
rf_feat_imps.plot(kind='bar')
plt.xlabel('features')
plt.ylabel('importance')
plt.title('Best random forest regressor feature importances');














# 'neg_mean_absolute_error' uses the (negative of) the mean absolute error
lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)


lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])
lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])
lr_mae_mean, lr_mae_std


mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))





rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)


rf_mae_mean = np.mean(-1 * rf_neg_mae['test_score'])
rf_mae_std = np.std(-1 * rf_neg_mae['test_score'])
rf_mae_mean, rf_mae_std


mean_absolute_error(y_test, rf_grid_cv.best_estimator_.predict(X_test))














fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, 1.0]
train_size, train_scores, test_scores = learning_curve(pipe, X_train, y_train, train_sizes=fractions)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)


plt.subplots(figsize=(10, 5))
plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)
plt.xlabel('Training set size')
plt.ylabel('CV scores')
plt.title('Cross-validation score as training set size increases');








#Code task 28#
#This may not be "production grade ML deployment" practice, but adding some basic
#information to your saved models can save your bacon in development.
#Just what version model have you just loaded to reuse? What version of `sklearn`
#created it? When did you make it?
#Assign the pandas version number (`pd.__version__`) to the `pandas_version` attribute,
#the numpy version (`np.__version__`) to the `numpy_version` attribute,
#the sklearn version (`sklearn_version`) to the `sklearn_version` attribute,
#and the current datetime (`datetime.datetime.now()`) to the `build_datetime` attribute
#Let's call this model version '1.0'
best_model = rf_grid_cv.best_estimator_
best_model.version = ___
best_model.pandas_version = ___
best_model.numpy_version = ___
best_model.sklearn_version = ___
best_model.X_columns = [col for col in X_train.columns]
best_model.build_datetime = ___


# save the model

modelpath = '../models'
save_file(best_model, 'ski_resort_pricing_model.pkl', modelpath)









